{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b21f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook (script) - 02_transform_delta\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "raw_path = \"s3a://my-youtube-bucket/raw/\"\n",
    "curated_path = \"s3a://my-youtube-bucket/curated/youtube/\"\n",
    "\n",
    "# Read raw JSON files\n",
    "df = spark.read.json(raw_path + \"*/*.json\")\n",
    "\n",
    "# Flatten and select fields\n",
    "cleaned = df.select(\n",
    "    F.coalesce(F.col(\"id.videoId\"), F.col(\"videoId\")).alias(\"video_id\"),\n",
    "    F.col(\"snippet.title\").alias(\"title\"),\n",
    "    F.col(\"snippet.channelTitle\").alias(\"channel_title\"),\n",
    "    F.to_timestamp(\"snippet.publishedAt\").alias(\"published_ts\")\n",
    ").where(F.col(\"video_id\").isNotNull())\n",
    "\n",
    "cleaned = cleaned.withColumn(\"year\", F.year(\"published_ts\")).withColumn(\"month\", F.month(\"published_ts\"))\n",
    "\n",
    "# Write to Delta partitioned by year/month\n",
    "(cleaned.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .save(curated_path))\n",
    "\n",
    "# Optimize (works in Databricks)\n",
    "try:\n",
    "    spark.sql(f\"OPTIMIZE delta.`{curated_path}` ZORDER BY (channel_title)\")\n",
    "except Exception as e:\n",
    "    print('OPTIMIZE skipped (not supported in this environment):', e)\n",
    "\n",
    "display(cleaned.limit(5))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
