# ğŸ¥ YouTube Data Engineering Project (AWS + Databricks)

![AWS](https://img.shields.io/badge/AWS-Cloud-orange?logo=amazon-aws&logoColor=white)
![Databricks](https://img.shields.io/badge/Databricks-BigData-red?logo=databricks)
![Python](https://img.shields.io/badge/Python-3.9-blue?logo=python)
![PySpark](https://img.shields.io/badge/PySpark-ETL-yellow?logo=apachespark)

## ğŸ“Œ Project Overview

This project demonstrates a **real-world cloud data pipeline** built using **AWS Services** and **Databricks** to process YouTube data at scale.

We implemented a **modern data lakehouse architecture** with **Bronze â†’ Silver â†’ Gold layers** powered by **Delta Lake** and orchestrated with **AWS Step Functions**.

The pipeline ingests YouTube metadata, stores raw data in S3, processes & optimizes it in Databricks, and exposes curated datasets for analytics.

---

## ğŸš€ Architecture Diagram

![Architecture](./docs/architecture.png)

**Workflow:**
1. **Ingestion** â†’ AWS Lambda fetches YouTube API data, streams via Kinesis, stores raw JSON in S3.  
2. **Raw Data Storage (Bronze)** â†’ Lambda saves ingested data in Delta format.  
3. **Orchestration** â†’ AWS Step Functions triggers Databricks ETL job via REST API.  
4. **Transformation (Silver)** â†’ Databricks cleans & flattens JSON, applies partitioning, and stores optimized Delta tables.  
5. **Optimizations** â†’ Delta Lake schema evolution, Z-Ordering, VACUUM.  
6. **Analytics (Gold)** â†’ Aggregations for reporting, dashboards, and queries via Databricks SQL / Athena.  
7. **Monitoring** â†’ CloudWatch + SNS for alerts & job monitoring.  

---

## ğŸ“‚ Project Structure

youtube-databricks-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ infra/
â”‚ â”œâ”€â”€ main.tf # Terraform infra
â”‚ â”œâ”€â”€ variables.tf
â”‚ â”œâ”€â”€ outputs.tf
â”‚ â””â”€â”€ iam_policies.tf
â”œâ”€â”€ lambda/
â”‚ â”œâ”€â”€ ingest_preprocess/
â”‚ â”œâ”€â”€ kinesis_consumer/
â”‚ â”œâ”€â”€ fetch_youtube_batch/
â”‚ â””â”€â”€ trigger_databricks_job/
â”œâ”€â”€ databricks/
â”‚ â”œâ”€â”€ notebooks/
â”‚ â”‚ â”œâ”€â”€ 01_ingest_raw.py
â”‚ â”‚ â”œâ”€â”€ 02_transform_delta.py
â”‚ â”‚ â”œâ”€â”€ 03_optimizations.py
â”‚ â”‚ â””â”€â”€ 04_reporting_queries.py
â”‚ â”œâ”€â”€ jobs/
â”‚ â”‚ â””â”€â”€ youtube_etl_job.json
â”œâ”€â”€ stepfunctions/
â”‚ â””â”€â”€ state_machine.asl.json
â”œâ”€â”€ cicd/
â”‚ â””â”€â”€ github-actions.yml
â””â”€â”€ docs/
â””â”€â”€ architecture.png

yaml
Copy code

---

## âš™ï¸ Technologies Used

- **AWS**
  - Lambda (serverless ingestion, Databricks trigger)
  - Kinesis Streams (real-time data ingestion)
  - S3 (data lake storage: raw/bronze/silver/gold)
  - DynamoDB (metadata storage)
  - Step Functions (orchestration)
  - SNS + CloudWatch (monitoring & notifications)

- **Databricks**
  - PySpark for ETL & data transformations
  - Delta Lake (schema evolution, Z-Order, VACUUM, Time Travel)
  - Databricks REST API (job triggers)
  - Databricks SQL (reporting queries)

- **Other**
  - Terraform (infra as code)
  - GitHub Actions (CI/CD)

---

## ğŸ”¨ Setup Instructions

### 1. Clone the repository
```bash
git clone https://github.com/<your-username>/youtube-databricks-pipeline.git
cd youtube-databricks-pipeline
2. Deploy Infrastructure
bash
Copy code
cd infra
terraform init
terraform apply
3. Deploy Lambda Functions
Each Lambda folder (lambda/ingest_preprocess, lambda/kinesis_consumer, etc.) has its own requirements.txt. Package and deploy:

bash
Copy code
cd lambda/ingest_preprocess
pip install -r requirements.txt -t .
zip -r function.zip .
aws lambda update-function-code --function-name ingest_preprocess --zip-file fileb://function.zip
4. Import Databricks Notebooks
Go to Databricks Workspace â†’ Repos â†’ Import each notebook from databricks/notebooks/.

5. Create Databricks Job
Use databricks/jobs/youtube_etl_job.json to create the job via API or UI.

6. Configure Step Functions
Upload stepfunctions/state_machine.asl.json into AWS Step Functions.

Link with Lambda for Databricks job trigger.

ğŸ“Š Example Queries (Databricks SQL)
sql
Copy code
-- Top 10 Channels by Video Count
SELECT channel, COUNT(*) AS video_count
FROM youtube_silver
GROUP BY channel
ORDER BY video_count DESC
LIMIT 10;

-- Trending Videos by Month
SELECT year, month, COUNT(video_id) AS videos
FROM youtube_silver
GROUP BY year, month
ORDER BY year DESC, month DESC;
âœ… Key Features
End-to-end serverless pipeline with AWS + Databricks.

Bronze â†’ Silver â†’ Gold data lakehouse architecture.

Delta Lake optimizations (Z-Order, schema evolution, vacuum).

Step Functions orchestration with retries & alerts.

Scalable & cloud-native design.

Analytics ready datasets.

ğŸ“Œ Future Improvements
Add CI/CD with Databricks Repos.

Implement real-time dashboards using Amazon QuickSight.

Add CDC (Change Data Capture) for incremental updates.

Integrate dbt for transformation lineage.

ğŸ§‘â€ğŸ’» Author
Pratik Pattanaik
Cloud & Data Engineer | AWS | Databricks | PySpark | SQL
